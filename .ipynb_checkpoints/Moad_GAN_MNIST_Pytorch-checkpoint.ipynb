{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Image generator\n",
    "    \n",
    "    Takes a noise vector as input and syntheses a single channel image accordingly\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dims, output_dims):\n",
    "        \"\"\"Init function\n",
    "        \n",
    "        Declare the network structure as indicated in CW2 Guidance\n",
    "        \n",
    "        Arguments:\n",
    "            input_dims {int} -- Dimension of input noise vector\n",
    "            output_dims {int} -- Dimension of the output vector (flatten image)\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        ###  TODO: Change the architecture and value as CW2 Guidance required\n",
    "        self.fc0 = nn.Sequential(\n",
    "            nn.Linear(input_dims, 128), \n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        # output hidden layer\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(128, output_dims), \n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function\n",
    "        \n",
    "        Arguments:\n",
    "            x {Tensor} -- a batch of noise vectors in shape (<batch_size>x<input_dims>)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor -- a batch of flatten image in shape (<batch_size>x<output_dims>)\n",
    "        \"\"\"\n",
    "        ###  TODO: modify to be consistent with the network structure\n",
    "        x = self.fc0(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Image discriminator\n",
    "    \n",
    "    Takes a image as input and predict if it is real from the dataset or fake synthesised by the generator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dims, output_dims=1):\n",
    "        \"\"\"Init function\n",
    "        \n",
    "        Declare the discriminator network structure as indicated in CW2 Guidance\n",
    "        \n",
    "        Arguments:\n",
    "            input_dims {int} -- Dimension of the flatten input images\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            output_dims {int} -- Predicted probability (default: {1})\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        ###  TODO: Change the architecture and value as CW2 Guidance required\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc0 = nn.Sequential(\n",
    "            nn.Linear(input_dims, 784),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(784, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function\n",
    "        \n",
    "        Arguments:\n",
    "            x {Tensor} -- a batch of 2D image in shape (<batch_size>xHxW)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor -- predicted probabilities (<batch_size>)\n",
    "        \"\"\"\n",
    "        ###  TODO: modify to be consistent with the network structure\n",
    "\n",
    "        x = self.fc0(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def show_result(G_net, z_, num_epoch, show=False, save=False, path='result.png'):\n",
    "    \"\"\"Result visualisation\n",
    "    \n",
    "    Show and save the generated figures in the grid fashion\n",
    "    \n",
    "    Arguments:\n",
    "        G_net {[nn.Module]} -- The generator instant\n",
    "        z_ {[Tensor]} -- Input noise vectors\n",
    "        num_epoch {[int]} -- Indicate how many epoch has the generator been trained\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        show {bool} -- If to display the images (default: {False})\n",
    "        save {bool} -- If to store the images (default: {False})\n",
    "        path {str} -- path to store the images (default: {'result.png'})\n",
    "    \"\"\"\n",
    "\n",
    "    ###  TODO: complete the rest of part\n",
    "    # hint: use plt.subplots to construct grid\n",
    "    # hint: use plt.imshow and plt.savefig to display and store the images\n",
    "    \n",
    "    show_data_fake = G_net(create_noise(b_size, G_input_dim).to(device))\n",
    "    fig, axes = plt.subplots(figsize=(7,7), nrows=5, ncols=5, sharey=True, sharex=True)\n",
    "    \n",
    "    for ax, img in zip(axes.flatten(), show_data_fake):\n",
    "        img = img.detach()\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
    "            \n",
    "    fig.savefig('path')\n",
    "\n",
    "def show_train_hist(hist, show=False, save=False, path='Train_hist.png'):\n",
    "    \"\"\"Loss tracker\n",
    "    \n",
    "    Plot the losses of generator and discriminator independently to see the trend\n",
    "    \n",
    "    Arguments:\n",
    "        hist {[dict]} -- Tracking variables\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        show {bool} -- If to display the figure (default: {False})\n",
    "        save {bool} -- If to store the figure (default: {False})\n",
    "        path {str} -- path to store the figure (default: {'Train_hist.png'})\n",
    "    \"\"\"\n",
    "    x = range(len(hist['D_losses']))\n",
    "\n",
    "    y1 = hist['D_losses']\n",
    "    y2 = hist['G_losses']\n",
    "\n",
    "    plt.plot(x, y1, label='D_loss')\n",
    "    plt.plot(x, y2, label='G_loss')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def create_noise(num, dim):\n",
    "    \"\"\"Noise constructor\n",
    "    \n",
    "    returns a tensor filled with random numbers from a standard normal distribution\n",
    "    \n",
    "    Arguments:\n",
    "        num {int} -- Number of vectors\n",
    "        dim {int} -- Dimension of vectors\n",
    "    \n",
    "    Returns:\n",
    "        [Tensor] -- the generated noise vector batch\n",
    "    \"\"\"\n",
    "    return torch.randn(num, dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # initialise the device for training, if gpu is available, device = 'cuda', else: device = 'cpu'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_dir = './MNIST_data/'\n",
    "    save_dir = './MNIST_GAN_results/'\n",
    "    image_save_dir = './MNIST_GAN_results/results'\n",
    "\n",
    "    # create folder if not exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    if not os.path.exists(image_save_dir):\n",
    "        os.mkdir(image_save_dir)\n",
    "\n",
    "    # training parameters\n",
    "    batch_size = 100\n",
    "    learning_rate = 0.0002\n",
    "    epochs = 100\n",
    "\n",
    "    # parameters for Models\n",
    "    image_size = 28\n",
    "    G_input_dim = 100\n",
    "    G_output_dim = image_size * image_size\n",
    "    D_input_dim = image_size * image_size\n",
    "    D_output_dim = 1\n",
    "    hidden_size = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # construct the dataset and data loader\n",
    "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "    train_data = datasets.MNIST(root=data_dir, train=True, transform=transform, download=True)\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:14<00:00, 41.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 with 14.41 s\n",
      "Generator loss: 1.59217050, Discriminator loss: 0.43449276\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-f165e78d591b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_save_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/MNIST_GAN_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.png'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mshow_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_noise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;31m# record the loss for every epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-473fc6fc36f4>\u001b[0m in \u001b[0;36mshow_result\u001b[1;34m(G_net, z_, num_epoch, show, save, path)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Greys_r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'path'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1447\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5521\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5523\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5524\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    696\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m             \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Needed e.g. to apply png palette.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 698\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_A\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msafe_masked_invalid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m         if (self._A.dtype != np.uint8 and\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\u001b[0m in \u001b[0;36msafe_masked_invalid\u001b[1;34m(x, copy)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msafe_masked_invalid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnative\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m         \u001b[1;31m# If we have already made a copy, do the byteswap in place, else make a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    619\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    622\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "    # declare the generator and discriminator networks    \n",
    "    G_net = Generator(G_input_dim, G_output_dim).to(device)\n",
    "    D_net = Discriminator(D_input_dim, D_output_dim).to(device)\n",
    "\n",
    "    # Binary Cross Entropy Loss function\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "\n",
    "    # Initialise the Optimizers\n",
    "    G_optimizer = torch.optim.Adam(G_net.parameters(), lr=learning_rate)\n",
    "    D_optimizer = torch.optim.Adam(D_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # tracking variables\n",
    "    train_hist = {}\n",
    "    train_hist['D_losses'] = []\n",
    "    train_hist['G_losses'] = []\n",
    "    train_hist['per_epoch_ptimes'] = []\n",
    "    train_hist['total_ptime'] = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        G_net.train()\n",
    "        D_net.train()\n",
    "        Loss_G = []\n",
    "        Loss_D = []\n",
    "        epoch_start_time = time.time()\n",
    "        for (image, _) in tqdm(train_loader):\n",
    "            image = image.to(device)\n",
    "            b_size = len(image)\n",
    "            # creat real and fake labels\n",
    "            real_label = torch.ones(b_size, 1).to(device)\n",
    "            fake_label = torch.zeros(b_size, 1).to(device)\n",
    "\n",
    "            # generate fake images \n",
    "            data_fake = G_net(create_noise(b_size, G_input_dim).to(device))\n",
    "            data_real = image.view(b_size, D_input_dim)\n",
    "\n",
    "            # --------train the discriminator network----------\n",
    "            # compute the loss for real and fake images\n",
    "            output_real = D_net(data_real)\n",
    "            output_fake = D_net(data_fake)\n",
    "            loss_real = criterion(output_real, real_label)\n",
    "            loss_fake = criterion(output_fake, fake_label)\n",
    "            loss_d = loss_real + loss_fake\n",
    "\n",
    "            # back propagation\n",
    "            D_optimizer.zero_grad()\n",
    "            loss_d.backward()\n",
    "            D_optimizer.step()\n",
    "\n",
    "            # -------- train the generator network-----------\n",
    "            data_fake = G_net(create_noise(b_size, G_input_dim).to(device))\n",
    "\n",
    "            # compute the loss for generator network\n",
    "            output_fake = D_net(data_fake)\n",
    "            loss_g = criterion(output_fake, real_label)\n",
    "\n",
    "            ## back propagation\n",
    "            G_optimizer.zero_grad()\n",
    "            loss_g.backward()\n",
    "            G_optimizer.step()\n",
    "\n",
    "            ## store the loss of each iter\n",
    "            Loss_D.append(loss_d.item())\n",
    "            Loss_G.append(loss_g.item())\n",
    "\n",
    "        epoch_loss_g = np.mean(Loss_G)  # mean generator loss for the epoch\n",
    "        epoch_loss_d = np.mean(Loss_D)  # mean discriminator loss for the epoch\n",
    "        epoch_end_time = time.time()\n",
    "        per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "\n",
    "        print(\"Epoch %d of %d with %.2f s\" % (epoch + 1, epochs, per_epoch_ptime))\n",
    "        print(\"Generator loss: %.8f, Discriminator loss: %.8f\" % (epoch_loss_g, epoch_loss_d))\n",
    "\n",
    "        path = image_save_dir + '/MNIST_GAN_' + str(epoch + 1) + '.png'\n",
    "        show_result(G_net, create_noise(25, 100).to(device), (epoch + 1), save=True, path=path)\n",
    "\n",
    "        # record the loss for every epoch\n",
    "        train_hist['G_losses'].append(epoch_loss_g)\n",
    "        train_hist['D_losses'].append(epoch_loss_d)\n",
    "        train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_ptime = end_time - start_time\n",
    "    train_hist['total_ptime'].append(total_ptime)\n",
    "\n",
    "    print('Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f' % (\n",
    "        np.mean(train_hist['per_epoch_ptimes']), epochs, total_ptime))\n",
    "    print(\"Training finish!... save training results\")\n",
    "    with open(save_dir + '/train_hist.pkl', 'wb') as f:\n",
    "        pickle.dump(train_hist, f)\n",
    "    show_train_hist(train_hist, save=True, path=save_dir + '/MNIST_GAN_train_hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
